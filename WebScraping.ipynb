{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5be2650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WebScraping\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import io\n",
    "from typing import Optional\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d8b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/118.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "}\n",
    "\n",
    "def build_url(year: int) -> str:\n",
    "    return f\"https://www.spotrac.com/mlb/injured/_/year/{year}/view/player\"\n",
    "\n",
    "def fetch_html(url: str, max_retries: int = 3, backoff: float = 2.0) -> Optional[str]:\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = requests.get(url, headers=HEADERS, timeout=20)\n",
    "            if resp.status_code == 200 and \"text/html\" in resp.headers.get(\"Content-Type\", \"\"):\n",
    "                return resp.text\n",
    "            if resp.status_code in (403, 429, 503):\n",
    "                time.sleep(backoff * attempt)\n",
    "                continue\n",
    "            break\n",
    "        except requests.RequestException:\n",
    "            time.sleep(backoff * attempt)\n",
    "    return None\n",
    "\n",
    "def parse_table_with_pandas(html: str) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        # Use StringIO to avoid the literal-HTML FutureWarning\n",
    "        tables = pd.read_html(io.StringIO(html))\n",
    "        if not tables:\n",
    "            return None\n",
    "        return max(tables, key=lambda t: t.shape[1])  # pick widest table\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def parse_table_with_bs4(html: str) -> Optional[pd.DataFrame]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    table = soup.find(\"table\")\n",
    "    if not table:\n",
    "        return None\n",
    "\n",
    "    thead = table.find(\"thead\")\n",
    "    if thead:\n",
    "        headers = [th.get_text(strip=True) for th in thead.find_all(\"th\")]\n",
    "    else:\n",
    "        first_row = table.find(\"tr\")\n",
    "        headers = [th.get_text(strip=True) for th in first_row.find_all([\"th\", \"td\"])] if first_row else []\n",
    "\n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\"):\n",
    "        tds = tr.find_all(\"td\")\n",
    "        if not tds:\n",
    "            continue\n",
    "        rows.append([td.get_text(\" \", strip=True) for td in tds])\n",
    "\n",
    "    if not rows:\n",
    "        return None\n",
    "\n",
    "    max_len = max(len(r) for r in rows)\n",
    "    if len(headers) != max_len:\n",
    "        if len(headers) < max_len:\n",
    "            headers += [f\"col_{i+1}\" for i in range(len(headers), max_len)]\n",
    "        else:\n",
    "            headers = headers[:max_len]\n",
    "\n",
    "    return pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "def clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip().replace(\"\\n\", \" \").replace(\"  \", \" \") for c in df.columns]\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    df = df.replace(\"\", pd.NA).dropna(how=\"all\")\n",
    "    return df\n",
    "\n",
    "def try_requests_then_playwright(url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    html = fetch_html(url)\n",
    "    if html:\n",
    "        for parser in (parse_table_with_pandas, parse_table_with_bs4):\n",
    "            df = parser(html)\n",
    "            if isinstance(df, pd.DataFrame) and not df.empty:\n",
    "                return clean_df(df)\n",
    "\n",
    "    # Fallback to Playwright (optional dependency)\n",
    "    try:\n",
    "        from playwright.sync_api import sync_playwright\n",
    "    except ImportError as e:\n",
    "        raise RuntimeError(\n",
    "            \"Requests parsing failed and Playwright is not installed.\\n\"\n",
    "            \"Install with:\\n  pip install playwright\\n  playwright install chromium\"\n",
    "        ) from e\n",
    "\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        context = browser.new_context(user_agent=HEADERS[\"User-Agent\"])\n",
    "        page = context.new_page()\n",
    "        page.goto(url, wait_until=\"domcontentloaded\", timeout=45000)\n",
    "        page.wait_for_selector(\"table\", timeout=20000)\n",
    "        content = page.content()\n",
    "        browser.close()\n",
    "\n",
    "    for parser in (parse_table_with_pandas, parse_table_with_bs4):\n",
    "        df = parser(content)\n",
    "        if isinstance(df, pd.DataFrame) and not df.empty:\n",
    "            return clean_df(df)\n",
    "\n",
    "    raise RuntimeError(\"Could not locate a data table on the page after rendering.\")\n",
    "\n",
    "# ---------------- Normalizer ----------------\n",
    "\n",
    "def _pick(df, *cands):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    cand_lc = [c.lower() for c in df.columns]\n",
    "    for want in cands:\n",
    "        for i, c in enumerate(cand_lc):\n",
    "            if want in c:\n",
    "                return df.columns[i]\n",
    "    return None\n",
    "\n",
    "def normalize_spotrac_injured_df(raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize Spotrac 'Injured List' table into a tidy, episode-level dataset.\n",
    "    - Each injury episode becomes its own row (including multiple date ranges per line).\n",
    "    \"\"\"\n",
    "    df = raw.copy()\n",
    "\n",
    "    col_rank   = _pick(df, \"rank\")\n",
    "    col_player = _pick(df, \"player\")\n",
    "    col_pos    = _pick(df, \"pos\")\n",
    "    col_team   = _pick(df, \"team\")\n",
    "    col_reason = _pick(df, \"reason\")\n",
    "    col_days   = _pick(df, \"days\")        # table's days (often season-total)\n",
    "    col_cash   = _pick(df, \"cash\", \"total\")\n",
    "\n",
    "    if any(c is None for c in [col_player, col_reason]):\n",
    "        raise ValueError(f\"Missing required columns. Found: {list(df.columns)}\")\n",
    "\n",
    "    # Canonicalize text columns\n",
    "    def _clean(s: pd.Series) -> pd.Series:\n",
    "        return s.astype(str).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "    if col_rank:\n",
    "        df[col_rank] = pd.to_numeric(df[col_rank], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[col_player] = _clean(df[col_player])\n",
    "    if col_pos:    df[col_pos] = _clean(df[col_pos])\n",
    "    if col_team:   df[col_team] = _clean(df[col_team])\n",
    "\n",
    "    # Keep original reason with possible newlines preserved\n",
    "    reason_raw = df[col_reason].astype(str)\n",
    "\n",
    "    # Regex to capture line-level \"IL type - injury : start-end\"\n",
    "    # We allow multiple such patterns per cell.\n",
    "    rx_line = re.compile(\n",
    "        r\"\"\"\n",
    "        (?P<il>[^:\\n,]+?(?:IL|List|Suspension|Restricted(?:\\s+List)?))   # IL type\n",
    "        (?:\\s*-\\s*(?P<inj>[^:\\n,]+?))?                                   # optional injury text\n",
    "        \\s*:\\s*\n",
    "        (?P<dates>.+?)                                                   # one or more date ranges until line/entry end\n",
    "        (?=$|\\n)                                                         # stop at end or newline\n",
    "        \"\"\",\n",
    "        re.IGNORECASE | re.VERBOSE\n",
    "    )\n",
    "\n",
    "    # Within a line, find all (start-end) date pairs (comma-separated allowed)\n",
    "    rx_range = re.compile(\n",
    "        r'(\\d{1,2}/\\d{1,2}/\\d{2})\\s*-\\s*(\\d{1,2}/\\d{1,2}/\\d{2})'\n",
    "    )\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        base = {\n",
    "            \"rank\":   row[col_rank] if col_rank else pd.NA,\n",
    "            \"player\": row[col_player],\n",
    "            \"pos\":    row[col_pos] if col_pos else pd.NA,\n",
    "            \"team\":   row[col_team] if col_team else pd.NA,\n",
    "        }\n",
    "        text = str(row[col_reason]).strip()\n",
    "\n",
    "        # Split by explicit newlines first (from preserved <br>s); also fall back to scanning entire string.\n",
    "        candidates = [t.strip() for t in re.split(r'\\n+', text) if t.strip()]\n",
    "        if not candidates:\n",
    "            candidates = [text]\n",
    "\n",
    "        matched_any = False\n",
    "        for cand in candidates:\n",
    "            # Try to find one or more IL entries inside this candidate\n",
    "            for m in rx_line.finditer(cand):\n",
    "                matched_any = True\n",
    "                il_type = (m.group(\"il\") or \"\").strip()\n",
    "                injury  = (m.group(\"inj\") or \"\").strip()\n",
    "                datestr = (m.group(\"dates\") or \"\").strip()\n",
    "\n",
    "                # Multiple date ranges in one line → explode\n",
    "                ranges = rx_range.findall(datestr) or [(None, None)]\n",
    "                for start_s, end_s in ranges:\n",
    "                    rec = dict(base)\n",
    "                    rec[\"il_type\"]    = il_type\n",
    "                    rec[\"injury\"]     = injury\n",
    "                    rec[\"reason_raw\"] = cand\n",
    "\n",
    "                    # Dates\n",
    "                    rec[\"start_date\"] = pd.to_datetime(start_s, format=\"%m/%d/%y\", errors=\"coerce\") if start_s else pd.NaT\n",
    "                    rec[\"end_date\"]   = pd.to_datetime(end_s,   format=\"%m/%d/%y\", errors=\"coerce\") if end_s else pd.NaT\n",
    "\n",
    "                    # Cash & days: keep totals off per-episode to avoid implying allocation\n",
    "                    # If you prefer to carry them through, uncomment below lines.\n",
    "                    # if col_days:\n",
    "                    #     rec[\"days_missed\"] = pd.to_numeric(\n",
    "                    #         str(row[col_days]).replace(\",\", \"\"),\n",
    "                    #         errors=\"coerce\"\n",
    "                    #     ).astype(\"Int64\")\n",
    "                    # if col_cash:\n",
    "                    #     cash = (\n",
    "                    #         str(row[col_cash])\n",
    "                    #         .replace(\"$\", \"\")\n",
    "                    #         .replace(\",\", \"\")\n",
    "                    #     )\n",
    "                    #     rec[\"cash_total\"] = pd.to_numeric(cash, errors=\"coerce\")\n",
    "\n",
    "                    records.append(rec)\n",
    "\n",
    "        # If we failed to match with rx_line at all, do a more permissive fallback:\n",
    "        if not matched_any:\n",
    "            # Try your original (rx1/rx2) single-extract logic as a last resort\n",
    "            rx1 = re.compile(\n",
    "                r\"^(?P<il>[^:]+?IL|[^:]+?List|Suspension|Restricted(?: List)?)\"\n",
    "                r\"(?:\\s*-\\s*(?P<inj>[^:]+?))?\"\n",
    "                r\"(?:\\s*:\\s*(?P<start>\\d{1,2}/\\d{1,2}/\\d{2}))?\"\n",
    "                r\"(?:-(?P<end>\\d{1,2}/\\d{1,2}/\\d{2}))?$\",\n",
    "                flags=re.IGNORECASE\n",
    "            )\n",
    "            rx2 = re.compile(\n",
    "                r\"^(?P<il>[^:]+?)\"\n",
    "                r\"(?:\\s*-\\s*(?P<inj>[^:]+?))?\"\n",
    "                r\"(?:\\s*:\\s*(?P<start>\\d{1,2}/\\d{1,2}/\\d{2}))?\"\n",
    "                r\"(?:-(?P<end>\\d{1,2}/\\d{1,2}/\\d{2}))?$\",\n",
    "                flags=re.IGNORECASE\n",
    "            )\n",
    "            ext = rx1.search(text) or rx2.search(text)\n",
    "            rec = dict(base)\n",
    "            rec[\"il_type\"]    = (ext.group(\"il\") if ext else \"\").strip()\n",
    "            rec[\"injury\"]     = (ext.group(\"inj\") if ext else \"\").strip()\n",
    "            rec[\"reason_raw\"] = text\n",
    "            rec[\"start_date\"] = pd.to_datetime(ext.group(\"start\"), format=\"%m/%d/%y\", errors=\"coerce\") if ext and ext.group(\"start\") else pd.NaT\n",
    "            rec[\"end_date\"]   = pd.to_datetime(ext.group(\"end\"),   format=\"%m/%d/%y\", errors=\"coerce\") if ext and ext.group(\"end\") else pd.NaT\n",
    "            records.append(rec)\n",
    "\n",
    "    out = pd.DataFrame.from_records(records)\n",
    "\n",
    "    # Optional numeric fields (commented in the loop) — if you want them, you can compute per-episode days:\n",
    "    # out[\"days_missed\"] = (out[\"end_date\"] - out[\"start_date\"]).dt.days.add(1)  # inclusive, if desired\n",
    "\n",
    "    # Order columns\n",
    "    want = [\"rank\", \"player\", \"pos\", \"team\",\n",
    "            \"il_type\", \"injury\", \"start_date\", \"end_date\", \"reason_raw\"]\n",
    "    return out[[c for c in want if c in out.columns]]\n",
    "\n",
    "\n",
    "# ---------------- Multi-year API (no autosave) ----------------\n",
    "\n",
    "def scrape_spotrac_years(years: List[int], sleep_sec: float = 1.0) -> Tuple[Dict[int, pd.DataFrame], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Scrape and normalize Spotrac MLB Injured List for multiple years.\n",
    "    Returns:\n",
    "      - tables: {year: cleaned DataFrame}\n",
    "      - combined: single DataFrame with a 'year' column\n",
    "    Does NOT write to disk.\n",
    "    \"\"\"\n",
    "    tables: Dict[int, pd.DataFrame] = {}\n",
    "    frames = []\n",
    "    for yr in years:\n",
    "        url = build_url(yr)\n",
    "        raw = try_requests_then_playwright(url)\n",
    "        clean = normalize_spotrac_injured_df(raw)\n",
    "        clean = clean.assign(year=yr)\n",
    "        tables[yr] = clean\n",
    "        frames.append(clean)\n",
    "        if sleep_sec:\n",
    "            time.sleep(sleep_sec)  # be polite to the site\n",
    "    combined = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "    return tables, combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a9bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>player</th>\n",
       "      <th>pos</th>\n",
       "      <th>team</th>\n",
       "      <th>il_type</th>\n",
       "      <th>injury</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>days_missed</th>\n",
       "      <th>cash_total</th>\n",
       "      <th>cash_per_day</th>\n",
       "      <th>reason_raw</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Anthony Rendon</td>\n",
       "      <td>3B</td>\n",
       "      <td>LAA</td>\n",
       "      <td>60-Day IL</td>\n",
       "      <td>Hip</td>\n",
       "      <td>2025-03-27</td>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>186</td>\n",
       "      <td>37999986</td>\n",
       "      <td>204301.0</td>\n",
       "      <td>60-Day IL - Hip: 3/27/25-9/28/25</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Gerrit Cole</td>\n",
       "      <td>SP</td>\n",
       "      <td>NYY</td>\n",
       "      <td>60-Day IL</td>\n",
       "      <td>Elbow Tommy John</td>\n",
       "      <td>2025-03-27</td>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>186</td>\n",
       "      <td>35999928</td>\n",
       "      <td>193548.0</td>\n",
       "      <td>60-Day IL - Elbow Tommy John: 3/27/25-9/28/25</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Kris Bryant</td>\n",
       "      <td>1B</td>\n",
       "      <td>COL</td>\n",
       "      <td>60-Day IL</td>\n",
       "      <td>Back</td>\n",
       "      <td>2025-04-13</td>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>169</td>\n",
       "      <td>23623665</td>\n",
       "      <td>139785.0</td>\n",
       "      <td>60-Day IL - Back: 4/13/25-9/28/25</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Jordan Montgomery</td>\n",
       "      <td>SP</td>\n",
       "      <td>ARI</td>\n",
       "      <td>60-Day IL</td>\n",
       "      <td>Elbow Tommy John</td>\n",
       "      <td>2025-03-27</td>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>186</td>\n",
       "      <td>22500048</td>\n",
       "      <td>120968.0</td>\n",
       "      <td>60-Day IL - Elbow Tommy John: 3/27/25-9/28/25</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Joe Musgrove</td>\n",
       "      <td>SP</td>\n",
       "      <td>SD</td>\n",
       "      <td>60-Day IL</td>\n",
       "      <td>Elbow Tommy John</td>\n",
       "      <td>2025-03-27</td>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>186</td>\n",
       "      <td>20000022</td>\n",
       "      <td>107527.0</td>\n",
       "      <td>60-Day IL - Elbow Tommy John: 3/27/25-9/28/25</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank             player pos team    il_type            injury start_date  \\\n",
       "0     1     Anthony Rendon  3B  LAA  60-Day IL               Hip 2025-03-27   \n",
       "1     2        Gerrit Cole  SP  NYY  60-Day IL  Elbow Tommy John 2025-03-27   \n",
       "2     3        Kris Bryant  1B  COL  60-Day IL              Back 2025-04-13   \n",
       "3     4  Jordan Montgomery  SP  ARI  60-Day IL  Elbow Tommy John 2025-03-27   \n",
       "4     5       Joe Musgrove  SP   SD  60-Day IL  Elbow Tommy John 2025-03-27   \n",
       "\n",
       "    end_date  days_missed  cash_total  cash_per_day  \\\n",
       "0 2025-09-28          186    37999986      204301.0   \n",
       "1 2025-09-28          186    35999928      193548.0   \n",
       "2 2025-09-28          169    23623665      139785.0   \n",
       "3 2025-09-28          186    22500048      120968.0   \n",
       "4 2025-09-28          186    20000022      107527.0   \n",
       "\n",
       "                                      reason_raw  year  \n",
       "0               60-Day IL - Hip: 3/27/25-9/28/25  2025  \n",
       "1  60-Day IL - Elbow Tommy John: 3/27/25-9/28/25  2025  \n",
       "2              60-Day IL - Back: 4/13/25-9/28/25  2025  \n",
       "3  60-Day IL - Elbow Tommy John: 3/27/25-9/28/25  2025  \n",
       "4  60-Day IL - Elbow Tommy John: 3/27/25-9/28/25  2025  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = [2025]\n",
    "tables, combined = scrape_spotrac_years(years)\n",
    "\n",
    "# Inspect\n",
    "{y: df.shape for y, df in tables.items()}\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3f3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv(\"mlb_injured_multi_year.csv\", index=False)\n",
    "# or per year:\n",
    "for y, df in tables.items():\n",
    "    df.to_parquet(f\"mlb_injured_{y}.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
