{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5be2650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WebScraping\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import io\n",
    "from typing import Optional\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74d8b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/118.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "}\n",
    "\n",
    "def build_url(year: int) -> str:\n",
    "    return f\"https://www.spotrac.com/mlb/injured/_/year/{year}/view/player\"\n",
    "\n",
    "def fetch_html(url: str, max_retries: int = 3, backoff: float = 2.0) -> Optional[str]:\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = requests.get(url, headers=HEADERS, timeout=20)\n",
    "            if resp.status_code == 200 and \"text/html\" in resp.headers.get(\"Content-Type\", \"\"):\n",
    "                return resp.text\n",
    "            if resp.status_code in (403, 429, 503):\n",
    "                time.sleep(backoff * attempt)\n",
    "                continue\n",
    "            break\n",
    "        except requests.RequestException:\n",
    "            time.sleep(backoff * attempt)\n",
    "    return None\n",
    "\n",
    "def parse_table_with_pandas(html: str) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        # Use StringIO to avoid the literal-HTML FutureWarning\n",
    "        tables = pd.read_html(io.StringIO(html))\n",
    "        if not tables:\n",
    "            return None\n",
    "        return max(tables, key=lambda t: t.shape[1])  # pick widest table\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def parse_table_with_bs4(html: str) -> Optional[pd.DataFrame]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    table = soup.find(\"table\")\n",
    "    if not table:\n",
    "        return None\n",
    "\n",
    "    thead = table.find(\"thead\")\n",
    "    if thead:\n",
    "        headers = [th.get_text(strip=True) for th in thead.find_all(\"th\")]\n",
    "    else:\n",
    "        first_row = table.find(\"tr\")\n",
    "        headers = [th.get_text(strip=True) for th in first_row.find_all([\"th\", \"td\"])] if first_row else []\n",
    "\n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\"):\n",
    "        tds = tr.find_all(\"td\")\n",
    "        if not tds:\n",
    "            continue\n",
    "        rows.append([td.get_text(\" \", strip=True) for td in tds])\n",
    "\n",
    "    if not rows:\n",
    "        return None\n",
    "\n",
    "    max_len = max(len(r) for r in rows)\n",
    "    if len(headers) != max_len:\n",
    "        if len(headers) < max_len:\n",
    "            headers += [f\"col_{i+1}\" for i in range(len(headers), max_len)]\n",
    "        else:\n",
    "            headers = headers[:max_len]\n",
    "\n",
    "    return pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "def clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip().replace(\"\\n\", \" \").replace(\"  \", \" \") for c in df.columns]\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    df = df.replace(\"\", pd.NA).dropna(how=\"all\")\n",
    "    return df\n",
    "\n",
    "def try_requests_then_playwright(url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    html = fetch_html(url)\n",
    "    if html:\n",
    "        for parser in (parse_table_with_pandas, parse_table_with_bs4):\n",
    "            df = parser(html)\n",
    "            if isinstance(df, pd.DataFrame) and not df.empty:\n",
    "                return clean_df(df)\n",
    "\n",
    "    # Fallback to Playwright (optional dependency)\n",
    "    try:\n",
    "        from playwright.sync_api import sync_playwright\n",
    "    except ImportError as e:\n",
    "        raise RuntimeError(\n",
    "            \"Requests parsing failed and Playwright is not installed.\\n\"\n",
    "            \"Install with:\\n  pip install playwright\\n  playwright install chromium\"\n",
    "        ) from e\n",
    "\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        context = browser.new_context(user_agent=HEADERS[\"User-Agent\"])\n",
    "        page = context.new_page()\n",
    "        page.goto(url, wait_until=\"domcontentloaded\", timeout=45000)\n",
    "        page.wait_for_selector(\"table\", timeout=20000)\n",
    "        content = page.content()\n",
    "        browser.close()\n",
    "\n",
    "    for parser in (parse_table_with_pandas, parse_table_with_bs4):\n",
    "        df = parser(content)\n",
    "        if isinstance(df, pd.DataFrame) and not df.empty:\n",
    "            return clean_df(df)\n",
    "\n",
    "    raise RuntimeError(\"Could not locate a data table on the page after rendering.\")\n",
    "\n",
    "# ---------------- Normalizer ----------------\n",
    "\n",
    "def _pick(df, *cands):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    cand_lc = [c.lower() for c in df.columns]\n",
    "    for want in cands:\n",
    "        for i, c in enumerate(cand_lc):\n",
    "            if want in c:\n",
    "                return df.columns[i]\n",
    "    return None\n",
    "\n",
    "def normalize_spotrac_injured_df(raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize Spotrac 'Injured List' table into a tidy, episode-level dataset.\n",
    "    - Each injury episode becomes its own row (including multiple date ranges per line).\n",
    "    \"\"\"\n",
    "    df = raw.copy()\n",
    "\n",
    "    col_rank   = _pick(df, \"rank\")\n",
    "    col_player = _pick(df, \"player\")\n",
    "    col_pos    = _pick(df, \"pos\")\n",
    "    col_team   = _pick(df, \"team\")\n",
    "    col_reason = _pick(df, \"reason\")\n",
    "    col_days   = _pick(df, \"days\")        # table's days (often season-total)\n",
    "    col_cash   = _pick(df, \"cash\", \"total\")\n",
    "\n",
    "    if any(c is None for c in [col_player, col_reason]):\n",
    "        raise ValueError(f\"Missing required columns. Found: {list(df.columns)}\")\n",
    "\n",
    "    # Canonicalize text columns\n",
    "    def _clean(s: pd.Series) -> pd.Series:\n",
    "        return s.astype(str).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "    if col_rank:\n",
    "        df[col_rank] = pd.to_numeric(df[col_rank], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[col_player] = _clean(df[col_player])\n",
    "    if col_pos:    df[col_pos] = _clean(df[col_pos])\n",
    "    if col_team:   df[col_team] = _clean(df[col_team])\n",
    "\n",
    "    # Keep original reason with possible newlines preserved\n",
    "    reason_raw = df[col_reason].astype(str)\n",
    "\n",
    "    # Regex to capture line-level \"IL type - injury : start-end\"\n",
    "    # We allow multiple such patterns per cell.\n",
    "    rx_line = re.compile(\n",
    "        r\"\"\"\n",
    "        (?P<il>[^:\\n,]+?(?:IL|List|Suspension|Restricted(?:\\s+List)?))   # IL type\n",
    "        (?:\\s*-\\s*(?P<inj>[^:\\n,]+?))?                                   # optional injury text\n",
    "        \\s*:\\s*\n",
    "        (?P<dates>.+?)                                                   # one or more date ranges until line/entry end\n",
    "        (?=$|\\n)                                                         # stop at end or newline\n",
    "        \"\"\",\n",
    "        re.IGNORECASE | re.VERBOSE\n",
    "    )\n",
    "\n",
    "    # Within a line, find all (start-end) date pairs (comma-separated allowed)\n",
    "    rx_range = re.compile(\n",
    "        r'(\\d{1,2}/\\d{1,2}/\\d{2})\\s*-\\s*(\\d{1,2}/\\d{1,2}/\\d{2})'\n",
    "    )\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        base = {\n",
    "            \"rank\":   row[col_rank] if col_rank else pd.NA,\n",
    "            \"player\": row[col_player],\n",
    "            \"pos\":    row[col_pos] if col_pos else pd.NA,\n",
    "            \"team\":   row[col_team] if col_team else pd.NA,\n",
    "        }\n",
    "        text = str(row[col_reason]).strip()\n",
    "\n",
    "        # Split by explicit newlines first (from preserved <br>s); also fall back to scanning entire string.\n",
    "        candidates = [t.strip() for t in re.split(r'\\n+', text) if t.strip()]\n",
    "        if not candidates:\n",
    "            candidates = [text]\n",
    "\n",
    "        matched_any = False\n",
    "        for cand in candidates:\n",
    "            # Try to find one or more IL entries inside this candidate\n",
    "            for m in rx_line.finditer(cand):\n",
    "                matched_any = True\n",
    "                il_type = (m.group(\"il\") or \"\").strip()\n",
    "                injury  = (m.group(\"inj\") or \"\").strip()\n",
    "                datestr = (m.group(\"dates\") or \"\").strip()\n",
    "\n",
    "                # Multiple date ranges in one line → explode\n",
    "                ranges = rx_range.findall(datestr) or [(None, None)]\n",
    "                for start_s, end_s in ranges:\n",
    "                    rec = dict(base)\n",
    "                    rec[\"il_type\"]    = il_type\n",
    "                    rec[\"injury\"]     = injury\n",
    "                    rec[\"reason_raw\"] = cand\n",
    "\n",
    "                    # Dates\n",
    "                    rec[\"start_date\"] = pd.to_datetime(start_s, format=\"%m/%d/%y\", errors=\"coerce\") if start_s else pd.NaT\n",
    "                    rec[\"end_date\"]   = pd.to_datetime(end_s,   format=\"%m/%d/%y\", errors=\"coerce\") if end_s else pd.NaT\n",
    "\n",
    "                    # Cash & days: keep totals off per-episode to avoid implying allocation\n",
    "                    # If you prefer to carry them through, uncomment below lines.\n",
    "                    # if col_days:\n",
    "                    #     rec[\"days_missed\"] = pd.to_numeric(\n",
    "                    #         str(row[col_days]).replace(\",\", \"\"),\n",
    "                    #         errors=\"coerce\"\n",
    "                    #     ).astype(\"Int64\")\n",
    "                    # if col_cash:\n",
    "                    #     cash = (\n",
    "                    #         str(row[col_cash])\n",
    "                    #         .replace(\"$\", \"\")\n",
    "                    #         .replace(\",\", \"\")\n",
    "                    #     )\n",
    "                    #     rec[\"cash_total\"] = pd.to_numeric(cash, errors=\"coerce\")\n",
    "\n",
    "                    records.append(rec)\n",
    "\n",
    "        # If we failed to match with rx_line at all, do a more permissive fallback:\n",
    "        if not matched_any:\n",
    "            # Try your original (rx1/rx2) single-extract logic as a last resort\n",
    "            rx1 = re.compile(\n",
    "                r\"^(?P<il>[^:]+?IL|[^:]+?List|Suspension|Restricted(?: List)?)\"\n",
    "                r\"(?:\\s*-\\s*(?P<inj>[^:]+?))?\"\n",
    "                r\"(?:\\s*:\\s*(?P<start>\\d{1,2}/\\d{1,2}/\\d{2}))?\"\n",
    "                r\"(?:-(?P<end>\\d{1,2}/\\d{1,2}/\\d{2}))?$\",\n",
    "                flags=re.IGNORECASE\n",
    "            )\n",
    "            rx2 = re.compile(\n",
    "                r\"^(?P<il>[^:]+?)\"\n",
    "                r\"(?:\\s*-\\s*(?P<inj>[^:]+?))?\"\n",
    "                r\"(?:\\s*:\\s*(?P<start>\\d{1,2}/\\d{1,2}/\\d{2}))?\"\n",
    "                r\"(?:-(?P<end>\\d{1,2}/\\d{1,2}/\\d{2}))?$\",\n",
    "                flags=re.IGNORECASE\n",
    "            )\n",
    "            ext = rx1.search(text) or rx2.search(text)\n",
    "            rec = dict(base)\n",
    "            rec[\"il_type\"]    = (ext.group(\"il\") if ext else \"\").strip()\n",
    "            rec[\"injury\"]     = (ext.group(\"inj\") if ext else \"\").strip()\n",
    "            rec[\"reason_raw\"] = text\n",
    "            rec[\"start_date\"] = pd.to_datetime(ext.group(\"start\"), format=\"%m/%d/%y\", errors=\"coerce\") if ext and ext.group(\"start\") else pd.NaT\n",
    "            rec[\"end_date\"]   = pd.to_datetime(ext.group(\"end\"),   format=\"%m/%d/%y\", errors=\"coerce\") if ext and ext.group(\"end\") else pd.NaT\n",
    "            records.append(rec)\n",
    "\n",
    "    out = pd.DataFrame.from_records(records)\n",
    "\n",
    "    # Optional numeric fields (commented in the loop) — if you want them, you can compute per-episode days:\n",
    "    # out[\"days_missed\"] = (out[\"end_date\"] - out[\"start_date\"]).dt.days.add(1)  # inclusive, if desired\n",
    "\n",
    "    # Order columns\n",
    "    want = [\"rank\", \"player\", \"pos\", \"team\",\n",
    "            \"il_type\", \"injury\", \"start_date\", \"end_date\", \"reason_raw\"]\n",
    "    return out[[c for c in want if c in out.columns]]\n",
    "\n",
    "\n",
    "# ---------------- Multi-year API (no autosave) ----------------\n",
    "\n",
    "def scrape_spotrac_years(years: List[int], sleep_sec: float = 1.0) -> Tuple[Dict[int, pd.DataFrame], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Scrape and normalize Spotrac MLB Injured List for multiple years.\n",
    "    Returns:\n",
    "      - tables: {year: cleaned DataFrame}\n",
    "      - combined: single DataFrame with a 'year' column\n",
    "    Does NOT write to disk.\n",
    "    \"\"\"\n",
    "    tables: Dict[int, pd.DataFrame] = {}\n",
    "    frames = []\n",
    "    for yr in years:\n",
    "        url = build_url(yr)\n",
    "        raw = try_requests_then_playwright(url)\n",
    "        clean = normalize_spotrac_injured_df(raw)\n",
    "        clean = clean.assign(year=yr)\n",
    "        tables[yr] = clean\n",
    "        frames.append(clean)\n",
    "        if sleep_sec:\n",
    "            time.sleep(sleep_sec)  # be polite to the site\n",
    "    combined = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "    return tables, combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f5a9bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>player</th>\n",
       "      <th>pos</th>\n",
       "      <th>team</th>\n",
       "      <th>il_type</th>\n",
       "      <th>injury</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>reason_raw</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Miguel Cabrera</td>\n",
       "      <td>DH</td>\n",
       "      <td>DET</td>\n",
       "      <td>10-Day IL</td>\n",
       "      <td>Hamstring</td>\n",
       "      <td>2018-05-04</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>10-Day IL - Hamstring: 5/4/18-6/1/18  10-Day I...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Miguel Cabrera</td>\n",
       "      <td>DH</td>\n",
       "      <td>DET</td>\n",
       "      <td>10-Day IL</td>\n",
       "      <td>Hamstring</td>\n",
       "      <td>2018-06-13</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>10-Day IL - Hamstring: 5/4/18-6/1/18  10-Day I...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Jacoby Ellsbury</td>\n",
       "      <td>CF</td>\n",
       "      <td>NYY</td>\n",
       "      <td>60-Day IL</td>\n",
       "      <td>Oblique</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>60-Day IL - Oblique: 3/29/18-10/1/18</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Yoenis Céspedes</td>\n",
       "      <td>LF</td>\n",
       "      <td>NYM</td>\n",
       "      <td>10-Day IL</td>\n",
       "      <td>Hip</td>\n",
       "      <td>2018-05-16</td>\n",
       "      <td>2018-07-20</td>\n",
       "      <td>10-Day IL - Hip: 5/16/18-7/20/18  60-Day IL - ...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Yoenis Céspedes</td>\n",
       "      <td>LF</td>\n",
       "      <td>NYM</td>\n",
       "      <td>10-Day IL</td>\n",
       "      <td>Hip</td>\n",
       "      <td>2018-07-24</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>10-Day IL - Hip: 5/16/18-7/20/18  60-Day IL - ...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6605</th>\n",
       "      <td>628</td>\n",
       "      <td>Cade Horton</td>\n",
       "      <td>SP</td>\n",
       "      <td>CHC</td>\n",
       "      <td>15-Day IL</td>\n",
       "      <td>Ribs</td>\n",
       "      <td>2025-09-25</td>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>15-Day IL - Ribs: 9/25/25-9/28/25</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6606</th>\n",
       "      <td>629</td>\n",
       "      <td>Everson Pereira</td>\n",
       "      <td>OF</td>\n",
       "      <td>TB</td>\n",
       "      <td>10-Day IL</td>\n",
       "      <td>Back</td>\n",
       "      <td>2025-09-26</td>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>10-Day IL - Back: 9/26/25-9/28/25</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6607</th>\n",
       "      <td>630</td>\n",
       "      <td>Brett Baty</td>\n",
       "      <td>2B</td>\n",
       "      <td>NYM</td>\n",
       "      <td>10-Day IL</td>\n",
       "      <td>Oblique</td>\n",
       "      <td>2025-09-27</td>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>10-Day IL - Oblique: 9/27/25-9/28/25</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6608</th>\n",
       "      <td>631</td>\n",
       "      <td>Nick Frasso</td>\n",
       "      <td>SP</td>\n",
       "      <td>LAD</td>\n",
       "      <td>60-Day IL</td>\n",
       "      <td>Undisclosed</td>\n",
       "      <td>2025-09-27</td>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>60-Day IL - Undisclosed: 9/27/25-9/28/25</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6609</th>\n",
       "      <td>632</td>\n",
       "      <td>Vaughn Grissom</td>\n",
       "      <td>2B</td>\n",
       "      <td>BOS</td>\n",
       "      <td>60-Day IL</td>\n",
       "      <td>Foot</td>\n",
       "      <td>2025-09-09</td>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>60-Day IL - Foot: 9/9/25-9/28/25</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6610 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rank           player pos team    il_type       injury start_date  \\\n",
       "0        1   Miguel Cabrera  DH  DET  10-Day IL    Hamstring 2018-05-04   \n",
       "1        1   Miguel Cabrera  DH  DET  10-Day IL    Hamstring 2018-06-13   \n",
       "2        2  Jacoby Ellsbury  CF  NYY  60-Day IL      Oblique 2018-03-29   \n",
       "3        3  Yoenis Céspedes  LF  NYM  10-Day IL          Hip 2018-05-16   \n",
       "4        3  Yoenis Céspedes  LF  NYM  10-Day IL          Hip 2018-07-24   \n",
       "...    ...              ...  ..  ...        ...          ...        ...   \n",
       "6605   628      Cade Horton  SP  CHC  15-Day IL         Ribs 2025-09-25   \n",
       "6606   629  Everson Pereira  OF   TB  10-Day IL         Back 2025-09-26   \n",
       "6607   630       Brett Baty  2B  NYM  10-Day IL      Oblique 2025-09-27   \n",
       "6608   631      Nick Frasso  SP  LAD  60-Day IL  Undisclosed 2025-09-27   \n",
       "6609   632   Vaughn Grissom  2B  BOS  60-Day IL         Foot 2025-09-09   \n",
       "\n",
       "       end_date                                         reason_raw  year  \n",
       "0    2018-06-01  10-Day IL - Hamstring: 5/4/18-6/1/18  10-Day I...  2018  \n",
       "1    2018-10-01  10-Day IL - Hamstring: 5/4/18-6/1/18  10-Day I...  2018  \n",
       "2    2018-10-01               60-Day IL - Oblique: 3/29/18-10/1/18  2018  \n",
       "3    2018-07-20  10-Day IL - Hip: 5/16/18-7/20/18  60-Day IL - ...  2018  \n",
       "4    2018-10-01  10-Day IL - Hip: 5/16/18-7/20/18  60-Day IL - ...  2018  \n",
       "...         ...                                                ...   ...  \n",
       "6605 2025-09-28                  15-Day IL - Ribs: 9/25/25-9/28/25  2025  \n",
       "6606 2025-09-28                  10-Day IL - Back: 9/26/25-9/28/25  2025  \n",
       "6607 2025-09-28               10-Day IL - Oblique: 9/27/25-9/28/25  2025  \n",
       "6608 2025-09-28           60-Day IL - Undisclosed: 9/27/25-9/28/25  2025  \n",
       "6609 2025-09-28                   60-Day IL - Foot: 9/9/25-9/28/25  2025  \n",
       "\n",
       "[6610 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# years = [2015, 2020, 2021, 2022, 2023, 2024, 2025]\n",
    "\n",
    "tables, combined = scrape_spotrac_years([i for i in range(2018, 2026)])\n",
    "\n",
    "# Inspect\n",
    "{y: df.shape for y, df in tables.items()}\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f3f3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv(\"mlb_injuries.csv\", index=False)\n",
    "# # or per year:\n",
    "# for y, df in tables.items():\n",
    "#     df.to_parquet(f\"mlb_injured_{y}.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a67b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
